-1.- Configurar proxy APT. Dentro del archivo /etc/apt/apt.conf agregar:
Acquire::http::Proxy "http://IP_PROXY:PUERTO_PROXY";

0.- Instalar VIM!
$ sudo aptitude install vim

1.- Instalar JDK (sólo necesario JRE para ejecutar)
$ sudo aptitude install openjdk-7-jdk

2.- Configurar variables de entorno JAVA_HOME. Dentro del ~/.bashrc
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-i386

3.- Descargar Hadoop y descomprimir en ~/
NOTA: Para descomprimir: 
$ tar xvfz hadoop-0.23.10.tar.gz

4.- Configurar variables de entorno HADOOP_INSTALL y PATH
export HADOOP_HOME=/home/hadoop/hadoop-0.23.10
export PATH=$PATH:$HADOOP_HOME/bin

4.1.- Verificar la instalación de Hadoop. Puede utilizar MapReduce como una instalación local a este punto
$ hadoop version


5.- Configurar Modo Pseudo-Distribuido. Dentro de $HADOOP_INSTALL/etc/hadoop

<?xml version="1.0"?>
<!-- core-site.xml -->
<configuration>
	<property>
		<name>fs.default.name</name>
		<value>hdfs://localhost/</value>
	</property>
</configuration>

<?xml version="1.0"?>
<!-- hdfs-site.xml -->
<configuration>
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
        <property>
                <name>dfs.name.dir</name>
                <value>file:///home/hadoop/dfs/name</value>
        </property>
        <property>
                <name>dfs.data.dir</name>
                <value>file:///home/hadoop/dfs/data</value>
        </property>
</configuration>

6.- Configuración SSH (authorized_keys):
$ sudo aptitude install openssh-server
$ ssh-keygen -t rsa 
(INTRO hasta el final)
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

6.1.- Para probar la instalación debe poder hacer ssh sin la necesidad de introducir clave
$ ssh localhost
$ exit 
(para salir de la shell remota)

7.- Preparando el Nodo de Nombres (namenode)

$ hdfs namenode -format

8.- Iniciando el servicio HDFS
$ hdfs namenode
$ hdfs datanode

